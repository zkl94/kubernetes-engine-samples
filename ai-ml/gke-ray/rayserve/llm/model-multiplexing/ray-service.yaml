from ray import serve
import vllm
import os

@serve.deployment(name="MultiModelDeployment")
class MultiModelDeployment:
    def __init__(self):
        # 模型配置字典，存储不同模型的配置
        self.model_configs = {
            "deepseek": {
                "model_id": os.environ["MODEL_1_ID"], # 从 env vars 读取模型 ID
                "quantize": os.environ["MODEL_1_QUANTIZE"], # 从 env vars 读取量化方式
                "tensor_parallel_size": int(os.environ["MODEL_1_TENSOR_PARALLELISM"]), # 从 env vars 读取 Tensor Parallelism 分片数
                "max_input_length": int(os.environ["MODEL_1_MAX_INPUT_TOKENS"]), # 从 env vars 读取最大输入 tokens
                "max_total_tokens": int(os.environ["MODEL_1_MAX_TOTAL_TOKENS"]), # 从 env vars 读取最大总 tokens
                "max_batch_size": 256, # 可以根据需要调整
                "max_num_seqs": 256, # 可以根据需要调整
            },
            "mistral": {
                "model_id": os.environ["MODEL_2_ID"], # 从 env vars 读取模型 ID
                "quantize": os.environ["MODEL_2_QUANTIZE"], # 从 env vars 读取量化方式
                "tensor_parallel_size": int(os.environ["MODEL_2_TENSOR_PARALLELISM"]), # 从 env vars 读取 Tensor Parallelism 分片数
                "max_input_length": int(os.environ["MODEL_2_MAX_INPUT_TOKENS"]), # 从 env vars 读取最大输入 tokens
                "max_total_tokens": int(os.environ["MODEL_2_MAX_TOTAL_TOKENS"]), # 从 env vars 读取最大总 tokens
                "max_batch_size": 256, # 可以根据需要调整
                "max_num_seqs": 256, # 可以根据需要调整
            },
            # 可以添加更多模型的配置 ...
        }
        self.llm_engines = {} # 存储已加载的 vLLM engine 实例

    def _get_llm_engine(self, model_name):
        """
        根据模型名称获取或创建 vLLM engine 实例
        """
        if model_name not in self.llm_engines:
            if model_name not in self.model_configs:
                raise ValueError(f"Model '{model_name}' not configured.")
            config = self.model_configs[model_name]
            self.llm_engines[model_name] = vllm.LLM(**config) # 创建 vLLM engine 实例
        return self.llm_engines[model_name]

    async def generate_deepseek(self, request: Request): # 处理 /generate_deepseek 请求
        """
        处理 DeepSeek 模型的生成请求
        """
        engine = self._get_llm_engine("deepseek") # 获取 DeepSeek 的 vLLM engine
        input_data = await request.json()
        messages = input_data.get("messages", [])

        # be compatible with openai format
        sampling_params_dict = input_data.get("sampling_params")
        if not sampling_params_dict: # 如果 "sampling_params" 字段不存在
          sampling_params_dict = input_data
        sampling_params = vllm.SamplingParams(**sampling_params_dict)
        
        result = engine.chat(messages, sampling_params) # 调用 vLLM engine 进行生成
        output = [r.outputs[0].text for r in result] # 提取生成的文本
        return {"text": output}

    async def generate_mistral(self, request: Request): # 处理 /generate_mistral 请求
        """
        处理 Mistral 模型的生成请求
        """
        engine = self._get_llm_engine("mistral") # 获取 DeepSeek 的 vLLM engine
        input_data = await request.json()
        messages = input_data.get("messages", [])
        
        # be compatible with openai format
        sampling_params_dict = input_data.get("sampling_params")
        if not sampling_params_dict: # 如果 "sampling_params" 字段不存在
          sampling_params_dict = input_data
        sampling_params = vllm.SamplingParams(**sampling_params_dict)
        
        result = engine.chat(messages, sampling_params) # 调用 vLLM engine 进行生成
        output = [r.outputs[0].text for r in result] # 提取生成的文本
        return {"text": output}

def multi_model(): # Ray Serve Application 入口函数
    deployment = MultiModelDeployment.bind() # 创建 MultiModelDeployment 的 Deployment 实例
    return {
        "/deepseek/v1/chat/completions": deployment.generate_deepseek.bind(), # 路由 /generate_deepseek 请求到 generate_deepseek 方法
        "/mistral/v1/chat/completions": deployment.generate_mistral.bind(), # 路由 /generate_mistral 请求到 generate_mistral 方法
    }
